{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"We make the most useful snippets easily available Use the Search Bar it works great! Submit your snippet, we'll publish it and give you credit.","title":"Welcome"},{"location":"#we-make-the-most-useful-snippets-easily-available","text":"Use the Search Bar it works great! Submit your snippet, we'll publish it and give you credit.","title":"We make the most useful snippets easily available"},{"location":"pandas/","text":"Drop Null Values Drop the rows in which ANY value is null : data_frame = data_frame.na.drop(\"any\") Drop null values only if ALL values are null in a row: df.na.drop(\"all\") Drop the null values ONLY in the selected columns df.na.drop(\"all\", Seq(\"col1\", \"col2\", \"col3\"))","title":"Pandas"},{"location":"pandas/#_1","text":"","title":""},{"location":"pandas/#drop-null-values","text":"","title":"Drop Null Values"},{"location":"pandas/#drop-the-rows-in-which-any-value-is-null","text":"data_frame = data_frame.na.drop(\"any\")","title":"Drop the rows in which ANY value is null:"},{"location":"pandas/#drop-null-values-only-if-all-values-are-null-in-a-row","text":"df.na.drop(\"all\")","title":"Drop null values only if ALL values are null in a row:"},{"location":"pandas/#drop-the-null-values-only-in-the-selected-columns","text":"df.na.drop(\"all\", Seq(\"col1\", \"col2\", \"col3\"))","title":"Drop the null values ONLY in the selected columns"},{"location":"pyspark/","text":"Create a New Table Once you have instantiated your spark context you can use its sql method to pass SQL commands. In this case we use the sql command CREATE TABLE IF NOT EXISTS to create a new database << my_database >> and a new table << my_table >>. Note how we must specify the name and the type of the new columns. database_name = 'my_database' table_name = 'my_table' # Make sure you specify the name and Type of each column spark.sql(f\"\"\" CREATE TABLE IF NOT EXISTS {database_name}.{table_name} (column_name_1 STRING, column_name_2 STRING, column_name_3 INT) \"\"\" ) Select Multiple Columns #Names of the columns to be selected array_of_columns = ['column_name_1', 'column_name_2', 'column_name_n' ] #Pass an array to the .select() method new_data_frame = data_frame.select([x for x in array_of_columns]) Show the Contents of a Column in a Dataframe without truncating df.show(df.count(), False) Get summary statistics from one specific column of a DataFrame result = df.select('column_name').describe().collect() Return the Mean of a Single Column from pyspark.sql.functions import mean as _mean, stddev as _stddev, col df_stats = df.select( _mean(col('total')).alias('mean'), _stddev(col('total')).alias('std') ).collect() mean = df_stats[0]['mean'] std = df_stats[0]['std'] print(mean, std) Insert Data Into Existing Table First make sure that your table exists. See \"Create a New Table\". Once your table exists you can proceed to insert data. Note that you need an active spark context. database_name = 'database_name' table_name = 'table_name' # Make sure your data follows the correct table structure # Respect the data types of each column: # In this case the table has 5 columns and we are inserting 4 new rows # Note how Col1 and Col 4 are type INT. Col2 and Col3 and Col5 are type STRING. spark.sql(f\"\"\" INSERT INTO TABLE {database_name}.{table_name} VALUES (11, 'Jon', 'Loyd', 25, 'Math'), (22, 'Frank', 'Mayer', 25, 'Math'), (33, 'Mary', 'Sten', 34, 'Chemistry'), (44, 'Arthur' , 'Welmer', 38, 'Chemistry'), \"\"\" ) Create a User Defined Function (UDF) First Import UDF and types . Then define your UDF just like you would define a function in python. Before using a UDF you have to register it with Spark and specify what type it will return. In this example we are returning an IntegerType() . If any other type is returned it will cause an error. Finally, to use a UDF , we call the method .withColumn() and specify a) the name of the new column where the output of the UDF will be deposited and b) the UDF to apply. The arguments of the UDF are the names of the existing DataFrame columns containing the data that the UDF will consume. # 1. Make sure you import udf and the correct type. from pyspark.sql.functions import udf # Can also use * to import ALL types instead of only IntegerType from pyspark.sql.types import IntegerType # 2. Create your UDF def user_defined_function(arg_1, arg_2): #arg_1 will correspond to existing_col_1 and arg_2 will correspond to existing_col_2 result = arg_1 + arg_2 return result # 3. Register your UDF with Spark my_spark_UDF = udf(user_defined_function, IntegerType()) # 4. Apply your UDF to a column and specify the name of the results column result_dataframe = data_frame.withColumn(\"result\", my_spark_UDF('existing_col_1', 'existing_col_2'))","title":"Pyspark"},{"location":"pyspark/#_1","text":"","title":""},{"location":"pyspark/#create-a-new-table","text":"Once you have instantiated your spark context you can use its sql method to pass SQL commands. In this case we use the sql command CREATE TABLE IF NOT EXISTS to create a new database << my_database >> and a new table << my_table >>. Note how we must specify the name and the type of the new columns. database_name = 'my_database' table_name = 'my_table' # Make sure you specify the name and Type of each column spark.sql(f\"\"\" CREATE TABLE IF NOT EXISTS {database_name}.{table_name} (column_name_1 STRING, column_name_2 STRING, column_name_3 INT) \"\"\" )","title":"Create a New Table"},{"location":"pyspark/#select-multiple-columns","text":"#Names of the columns to be selected array_of_columns = ['column_name_1', 'column_name_2', 'column_name_n' ] #Pass an array to the .select() method new_data_frame = data_frame.select([x for x in array_of_columns])","title":"Select Multiple Columns"},{"location":"pyspark/#show-the-contents-of-a-column-in-a-dataframe-without-truncating","text":"df.show(df.count(), False)","title":"Show the Contents of a Column in a Dataframe without truncating"},{"location":"pyspark/#get-summary-statistics-from-one-specific-column-of-a-dataframe","text":"result = df.select('column_name').describe().collect()","title":"Get summary statistics from one specific column of a DataFrame"},{"location":"pyspark/#return-the-mean-of-a-single-column","text":"from pyspark.sql.functions import mean as _mean, stddev as _stddev, col df_stats = df.select( _mean(col('total')).alias('mean'), _stddev(col('total')).alias('std') ).collect() mean = df_stats[0]['mean'] std = df_stats[0]['std'] print(mean, std)","title":"Return the Mean of a Single Column"},{"location":"pyspark/#insert-data-into-existing-table","text":"First make sure that your table exists. See \"Create a New Table\". Once your table exists you can proceed to insert data. Note that you need an active spark context. database_name = 'database_name' table_name = 'table_name' # Make sure your data follows the correct table structure # Respect the data types of each column: # In this case the table has 5 columns and we are inserting 4 new rows # Note how Col1 and Col 4 are type INT. Col2 and Col3 and Col5 are type STRING. spark.sql(f\"\"\" INSERT INTO TABLE {database_name}.{table_name} VALUES (11, 'Jon', 'Loyd', 25, 'Math'), (22, 'Frank', 'Mayer', 25, 'Math'), (33, 'Mary', 'Sten', 34, 'Chemistry'), (44, 'Arthur' , 'Welmer', 38, 'Chemistry'), \"\"\" )","title":"Insert Data Into Existing Table"},{"location":"pyspark/#create-a-user-defined-function-udf","text":"First Import UDF and types . Then define your UDF just like you would define a function in python. Before using a UDF you have to register it with Spark and specify what type it will return. In this example we are returning an IntegerType() . If any other type is returned it will cause an error. Finally, to use a UDF , we call the method .withColumn() and specify a) the name of the new column where the output of the UDF will be deposited and b) the UDF to apply. The arguments of the UDF are the names of the existing DataFrame columns containing the data that the UDF will consume. # 1. Make sure you import udf and the correct type. from pyspark.sql.functions import udf # Can also use * to import ALL types instead of only IntegerType from pyspark.sql.types import IntegerType # 2. Create your UDF def user_defined_function(arg_1, arg_2): #arg_1 will correspond to existing_col_1 and arg_2 will correspond to existing_col_2 result = arg_1 + arg_2 return result # 3. Register your UDF with Spark my_spark_UDF = udf(user_defined_function, IntegerType()) # 4. Apply your UDF to a column and specify the name of the results column result_dataframe = data_frame.withColumn(\"result\", my_spark_UDF('existing_col_1', 'existing_col_2'))","title":"Create a User Defined Function (UDF)"},{"location":"python/","text":"Install a Virtual Environment #1. Install virtual env pip install virtualenv #2. Create a virual environment in the folder of your choice cd your_folder virtualenv name_of_your_virtual_environment #3. Activate your virtual environment source name_of_your_virtual_environment/bin/activate #4 Now you can install pip packages in your virtual environment pip install your_new_package #5. When done, deactivate your virtual environment. You need to be in your_folder directory. deactivate #6. To delete your virtual environment just delete your_folder directory rm -rf name_of_your_virtual_environment #7 To list all your virtual environments lsvirtualenv Reference: https://python-guide-ru.readthedocs.io/en/latest/dev/virtualenvs.html List files in current directory import os #Collect all files in present working directory files = [file for file in os.listdir('.') if os.path.isfile(file)] #Print all files for file in files: print(file) If you need to list files in a directory other than the present working directory: files = [file for file in os.path.isfile(os.path.join(another_directory, file))]","title":"Python"},{"location":"python/#_1","text":"","title":""},{"location":"python/#install-a-virtual-environment","text":"#1. Install virtual env pip install virtualenv #2. Create a virual environment in the folder of your choice cd your_folder virtualenv name_of_your_virtual_environment #3. Activate your virtual environment source name_of_your_virtual_environment/bin/activate #4 Now you can install pip packages in your virtual environment pip install your_new_package #5. When done, deactivate your virtual environment. You need to be in your_folder directory. deactivate #6. To delete your virtual environment just delete your_folder directory rm -rf name_of_your_virtual_environment #7 To list all your virtual environments lsvirtualenv Reference: https://python-guide-ru.readthedocs.io/en/latest/dev/virtualenvs.html","title":"Install a Virtual Environment"},{"location":"python/#list-files-in-current-directory","text":"import os #Collect all files in present working directory files = [file for file in os.listdir('.') if os.path.isfile(file)] #Print all files for file in files: print(file) If you need to list files in a directory other than the present working directory: files = [file for file in os.path.isfile(os.path.join(another_directory, file))]","title":"List files in current directory"}]}