{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"We publish the most useful code snippets ! Please share us with friends !","title":"Welcome"},{"location":"#we-publish-the-most-useful-code-snippets","text":"","title":"We publish the most useful code snippets !"},{"location":"#please-share-us-with-friends","text":"","title":"Please share us with friends  !"},{"location":"pandas/","text":"Display ALL the rows of a Pandas DataFrame 1 pd . set_option ( 'display.max_row' , None ) Display ALL the contents in the rows of a Pandas DataFrame 1 pd . set_option ( 'display.max_colwidth' , - 1 ) Drop Null Values Drop the rows in which ANY value is null : 1 data_frame = data_frame . na . drop ( \"any\" ) Drop null values only if ALL values are null in a row: 1 df . na . drop ( \"all\" ) Percentage of a DataFrame 1 df1 [ 'percentage' ] = df1 [ 'Mathematics_score' ] / df1 [ 'Mathematics_score' ] . sum () Round values in data_frame 1 2 x [ 'percentage' ] = ( x [ 'count' ] / x [ 'count' ] . sum ()) * 100 x_1 = x . sort_values ( by = [ 'round(all_dates_fraction_greater_than_zero_reqs, 2)' ]) Drop the null values ONLY in the selected columns 1 df . na . drop ( \"all\" , Seq ( \"col1\" , \"col2\" , \"col3\" )) Select Multiple Columns From Existing Dataframe To select only certain columns from an existing Dataframe just pass it a list of columns 1 2 list_of_columns = [ 'col_name_1' , 'col_name_2' , 'col_name_n' ] df = df [ 'list_of_columns' ] New column with value depending on multiple conditions 1 2 3 4 5 6 7 8 9 10 11 12 # Create list with the conditions conditions = [ ( df [ 'col_name' ] < 0 ), ( df [ 'col_name' ] == 0 ), ( df [ 'col_name' ] > 0 ) ] # Create a list with the values corresponding to each condition values = [ 'blue' , 'white' , 'red' ] # New column 'color' whose value depends on the conditions in the list above df [ 'color' ] = np . select ( conditions , values ) Filter DataFrame based on a column values 1 new_df = df [ df . col_name == 'condition' ]","title":"Pandas"},{"location":"pandas/#_1","text":"","title":""},{"location":"pandas/#display-all-the-rows-of-a-pandas-dataframe","text":"1 pd . set_option ( 'display.max_row' , None )","title":"Display ALL the rows of a Pandas DataFrame"},{"location":"pandas/#display-all-the-contents-in-the-rows-of-a-pandas-dataframe","text":"1 pd . set_option ( 'display.max_colwidth' , - 1 )","title":"Display ALL the contents in the rows of a Pandas DataFrame"},{"location":"pandas/#drop-null-values","text":"","title":"Drop Null Values"},{"location":"pandas/#drop-the-rows-in-which-any-value-is-null","text":"1 data_frame = data_frame . na . drop ( \"any\" )","title":"Drop the rows in which ANY value is null:"},{"location":"pandas/#drop-null-values-only-if-all-values-are-null-in-a-row","text":"1 df . na . drop ( \"all\" )","title":"Drop null values only if ALL values are null in a row:"},{"location":"pandas/#percentage-of-a-dataframe","text":"1 df1 [ 'percentage' ] = df1 [ 'Mathematics_score' ] / df1 [ 'Mathematics_score' ] . sum ()","title":"Percentage of a DataFrame"},{"location":"pandas/#round-values-in-data_frame","text":"1 2 x [ 'percentage' ] = ( x [ 'count' ] / x [ 'count' ] . sum ()) * 100 x_1 = x . sort_values ( by = [ 'round(all_dates_fraction_greater_than_zero_reqs, 2)' ])","title":"Round values  in data_frame"},{"location":"pandas/#drop-the-null-values-only-in-the-selected-columns","text":"1 df . na . drop ( \"all\" , Seq ( \"col1\" , \"col2\" , \"col3\" ))","title":"Drop the null values ONLY in the selected columns"},{"location":"pandas/#select-multiple-columns-from-existing-dataframe","text":"To select only certain columns from an existing Dataframe just pass it a list of columns 1 2 list_of_columns = [ 'col_name_1' , 'col_name_2' , 'col_name_n' ] df = df [ 'list_of_columns' ]","title":"Select Multiple Columns From Existing Dataframe"},{"location":"pandas/#new-column-with-value-depending-on-multiple-conditions","text":"1 2 3 4 5 6 7 8 9 10 11 12 # Create list with the conditions conditions = [ ( df [ 'col_name' ] < 0 ), ( df [ 'col_name' ] == 0 ), ( df [ 'col_name' ] > 0 ) ] # Create a list with the values corresponding to each condition values = [ 'blue' , 'white' , 'red' ] # New column 'color' whose value depends on the conditions in the list above df [ 'color' ] = np . select ( conditions , values )","title":"New column with value depending on multiple conditions"},{"location":"pandas/#filter-dataframe-based-on-a-column-values","text":"1 new_df = df [ df . col_name == 'condition' ]","title":"Filter DataFrame based on a column values"},{"location":"pyspark/","text":"Create a New Table Once you have instantiated your spark context you can use its sql method to pass SQL commands. In this case we use the sql command CREATE TABLE IF NOT EXISTS to create a new database << my_database >> and a new table << my_table >>. Note how we must specify the name and the type of the new columns. 1 2 3 4 5 6 7 8 database_name = 'my_database' table_name = 'my_table' #Make sure you specify the name and Type of each column spark . sql ( f \"\"\" CREATE TABLE IF NOT EXISTS {database_name}.{table_name} (column_name_1 STRING, column_name_2 STRING, column_name_3 INT) \"\"\" ) Select Multiple Columns 1 2 3 4 5 # Names of the columns to be selected array_of_columns = [ 'column_name_1' , 'column_name_2' , 'column_name_n' ] # Pass an array to the .select() method new_data_frame = data_frame . select ([ x for x in array_of_columns ]) Display the Contents of Select Columns Only 1 2 3 4 5 array_of_columns = [ 'column_name_1' , 'col_name_2' ] df . select ( array_of_columns ) . show () #Can include a .describe() method if you want the stats for that array_of_columns df . select ( array_of_columns ) . describe () . show () Display the Data Types of all Columns 1 dataframe . dtypes Show the Contents of a Column in a Dataframe without truncating 1 df . show ( df . count (), False ) Get summary statistics from one specific column of a DataFrame 1 result = df . select ( 'column_name' ) . describe () . collect () Create a Histogram 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # Use Spark's `histogram` function from the RDD API number_of_bins = 5 my_histogram = ( data_frame . select ( 'column_name' ) . rdd . flatMap ( lambda record : record ) . histogram ( number_of_bins ) ) # Load the my_historgam into a Pandas DataFrame for plotting pd . DataFrame ( list ( zip ( * my_histogram )), columns = [ 'bin' , 'frequency' ] ) . set_index ( 'bin' ) . plot ( kind = 'bar' ) Left Join two DataFrames 1 result = df1 . join ( df2 , on = [ 'column_name_to_join' ], how = 'left' ) Count number of NaN's or Null's in DataFrames 1 2 3 from pyspark.sql.functions import isnan , when , count , col fraction_engaged . select ([ count ( when ( isnan ( x ) | col ( x ) . isNull (), x )) . alias ( x ) for x in fraction_engaged . columns ]) . show () Count the number of duplicates in a DataFrames 1 2 3 4 5 6 7 import pyspark.sql.functions as f df . groupBy ( df . columns ) \\ . count () \\ . where ( f . col ( 'count' ) > 1 ) \\ . select ( f . sum ( 'count' )) \\ . show () Take Random Samples from a DataFrame 1 2 3 4 5 6 7 8 9 #Limit the number of samples to 2000 N = 2000 #Specify the number of samples as a fraction of the total fraction = 0.5 samples = df . sample ( False , fraction , seed = 0 ) . limit ( N ) #Double check the total samples . count () Return the Mean of a Single Column 1 2 3 4 5 6 7 8 9 10 11 from pyspark.sql.functions import mean as _mean , stddev as _stddev , col df_stats = df . select ( _mean ( col ( 'total' )) . alias ( 'mean' ), _stddev ( col ( 'total' )) . alias ( 'std' ) ) . collect () mean = df_stats [ 0 ][ 'mean' ] std = df_stats [ 0 ][ 'std' ] print ( mean , std ) Insert Data Into Existing Table First make sure that your table exists. See \"Create a New Table\". Once your table exists you can proceed to insert data. Note that you need an active spark context. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 database_name = 'database_name' table_name = 'table_name' # Make sure your data follows the correct table structure # Respect the data types of each column: # In this case the table has 5 columns and we are inserting 4 new rows # Note how Col1 and Col 4 are type INT. Col2 and Col3 and Col5 are type STRING. spark . sql ( f \"\"\" INSERT INTO TABLE {database_name}.{table_name} VALUES (11, 'Jon', 'Loyd', 25, 'Math'), (22, 'Frank', 'Mayer', 25, 'Math'), (33, 'Mary', 'Sten', 34, 'Chemistry'), (44, 'Arthur' , 'Welmer', 38, 'Chemistry'), \"\"\" ) Create a User Defined Function (UDF) First Import UDF and types . Then define your UDF just like you would define a function in python. Before using a UDF you have to register it with Spark and specify what type it will return. In this example we are returning an IntegerType() . If any other type is returned it will cause an error. Finally, to use a UDF , we call the method .withColumn() and specify a) the name of the new column where the output of the UDF will be deposited and b) the UDF to apply. The arguments of the UDF are the names of the existing DataFrame columns containing the data that the UDF will consume. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # 1. Make sure you import udf and the correct type. from pyspark.sql.functions import udf # Can also use * to import ALL types instead of only IntegerType from pyspark.sql.types import IntegerType # 2. Create your UDF def user_defined_function ( arg_1 , arg_2 ): #arg_1 will correspond to existing_col_1 and arg_2 will correspond to existing_col_2 result = arg_1 + arg_2 return result # 3. Register your UDF with Spark my_spark_UDF = udf ( user_defined_function , IntegerType ()) # 4. Apply your UDF to a column and specify the name of the results column result_dataframe = data_frame . withColumn ( \"result\" , my_spark_UDF ( 'existing_col_1' , 'existing_col_2' ))","title":"Pyspark"},{"location":"pyspark/#_1","text":"","title":""},{"location":"pyspark/#create-a-new-table","text":"Once you have instantiated your spark context you can use its sql method to pass SQL commands. In this case we use the sql command CREATE TABLE IF NOT EXISTS to create a new database << my_database >> and a new table << my_table >>. Note how we must specify the name and the type of the new columns. 1 2 3 4 5 6 7 8 database_name = 'my_database' table_name = 'my_table' #Make sure you specify the name and Type of each column spark . sql ( f \"\"\" CREATE TABLE IF NOT EXISTS {database_name}.{table_name} (column_name_1 STRING, column_name_2 STRING, column_name_3 INT) \"\"\" )","title":"Create a New Table"},{"location":"pyspark/#select-multiple-columns","text":"1 2 3 4 5 # Names of the columns to be selected array_of_columns = [ 'column_name_1' , 'column_name_2' , 'column_name_n' ] # Pass an array to the .select() method new_data_frame = data_frame . select ([ x for x in array_of_columns ])","title":"Select Multiple Columns"},{"location":"pyspark/#display-the-contents-of-select-columns-only","text":"1 2 3 4 5 array_of_columns = [ 'column_name_1' , 'col_name_2' ] df . select ( array_of_columns ) . show () #Can include a .describe() method if you want the stats for that array_of_columns df . select ( array_of_columns ) . describe () . show ()","title":"Display the Contents of Select Columns Only"},{"location":"pyspark/#display-the-data-types-of-all-columns","text":"1 dataframe . dtypes","title":"Display the Data Types of all Columns"},{"location":"pyspark/#show-the-contents-of-a-column-in-a-dataframe-without-truncating","text":"1 df . show ( df . count (), False )","title":"Show the Contents of a Column in a Dataframe without truncating"},{"location":"pyspark/#get-summary-statistics-from-one-specific-column-of-a-dataframe","text":"1 result = df . select ( 'column_name' ) . describe () . collect ()","title":"Get summary statistics from one specific column of a DataFrame"},{"location":"pyspark/#create-a-histogram","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # Use Spark's `histogram` function from the RDD API number_of_bins = 5 my_histogram = ( data_frame . select ( 'column_name' ) . rdd . flatMap ( lambda record : record ) . histogram ( number_of_bins ) ) # Load the my_historgam into a Pandas DataFrame for plotting pd . DataFrame ( list ( zip ( * my_histogram )), columns = [ 'bin' , 'frequency' ] ) . set_index ( 'bin' ) . plot ( kind = 'bar' )","title":"Create a Histogram"},{"location":"pyspark/#left-join-two-dataframes","text":"1 result = df1 . join ( df2 , on = [ 'column_name_to_join' ], how = 'left' )","title":"Left Join two DataFrames"},{"location":"pyspark/#count-number-of-nans-or-nulls-in-dataframes","text":"1 2 3 from pyspark.sql.functions import isnan , when , count , col fraction_engaged . select ([ count ( when ( isnan ( x ) | col ( x ) . isNull (), x )) . alias ( x ) for x in fraction_engaged . columns ]) . show ()","title":"Count number of NaN's or Null's in DataFrames"},{"location":"pyspark/#count-the-number-of-duplicates-in-a-dataframes","text":"1 2 3 4 5 6 7 import pyspark.sql.functions as f df . groupBy ( df . columns ) \\ . count () \\ . where ( f . col ( 'count' ) > 1 ) \\ . select ( f . sum ( 'count' )) \\ . show ()","title":"Count the number of duplicates in a DataFrames"},{"location":"pyspark/#take-random-samples-from-a-dataframe","text":"1 2 3 4 5 6 7 8 9 #Limit the number of samples to 2000 N = 2000 #Specify the number of samples as a fraction of the total fraction = 0.5 samples = df . sample ( False , fraction , seed = 0 ) . limit ( N ) #Double check the total samples . count ()","title":"Take Random Samples from a DataFrame"},{"location":"pyspark/#return-the-mean-of-a-single-column","text":"1 2 3 4 5 6 7 8 9 10 11 from pyspark.sql.functions import mean as _mean , stddev as _stddev , col df_stats = df . select ( _mean ( col ( 'total' )) . alias ( 'mean' ), _stddev ( col ( 'total' )) . alias ( 'std' ) ) . collect () mean = df_stats [ 0 ][ 'mean' ] std = df_stats [ 0 ][ 'std' ] print ( mean , std )","title":"Return the Mean of a Single Column"},{"location":"pyspark/#insert-data-into-existing-table","text":"First make sure that your table exists. See \"Create a New Table\". Once your table exists you can proceed to insert data. Note that you need an active spark context. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 database_name = 'database_name' table_name = 'table_name' # Make sure your data follows the correct table structure # Respect the data types of each column: # In this case the table has 5 columns and we are inserting 4 new rows # Note how Col1 and Col 4 are type INT. Col2 and Col3 and Col5 are type STRING. spark . sql ( f \"\"\" INSERT INTO TABLE {database_name}.{table_name} VALUES (11, 'Jon', 'Loyd', 25, 'Math'), (22, 'Frank', 'Mayer', 25, 'Math'), (33, 'Mary', 'Sten', 34, 'Chemistry'), (44, 'Arthur' , 'Welmer', 38, 'Chemistry'), \"\"\" )","title":"Insert Data Into Existing Table"},{"location":"pyspark/#create-a-user-defined-function-udf","text":"First Import UDF and types . Then define your UDF just like you would define a function in python. Before using a UDF you have to register it with Spark and specify what type it will return. In this example we are returning an IntegerType() . If any other type is returned it will cause an error. Finally, to use a UDF , we call the method .withColumn() and specify a) the name of the new column where the output of the UDF will be deposited and b) the UDF to apply. The arguments of the UDF are the names of the existing DataFrame columns containing the data that the UDF will consume. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # 1. Make sure you import udf and the correct type. from pyspark.sql.functions import udf # Can also use * to import ALL types instead of only IntegerType from pyspark.sql.types import IntegerType # 2. Create your UDF def user_defined_function ( arg_1 , arg_2 ): #arg_1 will correspond to existing_col_1 and arg_2 will correspond to existing_col_2 result = arg_1 + arg_2 return result # 3. Register your UDF with Spark my_spark_UDF = udf ( user_defined_function , IntegerType ()) # 4. Apply your UDF to a column and specify the name of the results column result_dataframe = data_frame . withColumn ( \"result\" , my_spark_UDF ( 'existing_col_1' , 'existing_col_2' ))","title":"Create a User Defined Function (UDF)"},{"location":"python/","text":"Install a Virtual Environment 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 #1. Install virtual env pip install virtualenv #2. Create a virual environment in the folder of your choice cd your_folder virtualenv name_of_your_virtual_environment #3. Activate your virtual environment source name_of_your_virtual_environment / bin / activate #4 Now you can install pip packages in your virtual environment pip install your_new_package #5. When done, deactivate your virtual environment. You need to be in your_folder directory. deactivate #6. To delete your virtual environment just delete your_folder directory rm - rf name_of_your_virtual_environment #7 To list all your virtual environments lsvirtualenv Reference: https://python-guide-ru.readthedocs.io/en/latest/dev/virtualenvs.html List files in current directory 1 2 3 4 5 6 7 8 import os #Collect all files in present working directory files = [ file for file in os . listdir ( '.' ) if os . path . isfile ( file )] #Print all files for file in files : print ( file ) If you need to list files in a directory other than the present working directory: 1 files = [ file for file in os . path . isfile ( os . path . join ( another_directory , file ))]","title":"Python"},{"location":"python/#_1","text":"","title":""},{"location":"python/#install-a-virtual-environment","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 #1. Install virtual env pip install virtualenv #2. Create a virual environment in the folder of your choice cd your_folder virtualenv name_of_your_virtual_environment #3. Activate your virtual environment source name_of_your_virtual_environment / bin / activate #4 Now you can install pip packages in your virtual environment pip install your_new_package #5. When done, deactivate your virtual environment. You need to be in your_folder directory. deactivate #6. To delete your virtual environment just delete your_folder directory rm - rf name_of_your_virtual_environment #7 To list all your virtual environments lsvirtualenv Reference: https://python-guide-ru.readthedocs.io/en/latest/dev/virtualenvs.html","title":"Install a Virtual Environment"},{"location":"python/#list-files-in-current-directory","text":"1 2 3 4 5 6 7 8 import os #Collect all files in present working directory files = [ file for file in os . listdir ( '.' ) if os . path . isfile ( file )] #Print all files for file in files : print ( file ) If you need to list files in a directory other than the present working directory: 1 files = [ file for file in os . path . isfile ( os . path . join ( another_directory , file ))]","title":"List files in current directory"},{"location":"sql/","text":"Calculate difference in dates (HIVE SQL) 1 2 3 4 datediff ( to_date ( String column_with_timestamp_1 ), to_date ( String column_with_timestamp_1 ) ) List all columns in a table (SQL) 1 DESCRIBE name_of_table ; IF clause (SQL) 1 IF ( col_name IS NULL , 'Missing' , col_name ) AS new_col_name , CASE clause (SQL) 1 2 3 4 5 6 CASE WHEN column_name_1 = 'condition_1' THEN 'result_1' WHEN column_name_2 LIKE 'begins_with_some_text%' THEN 'result_2' WHEN column_name_3 LIKE 'begins_with_some_text%' THEN 'result_3' ELSE 'Other' END AS new_column_name ,","title":"SQL"},{"location":"sql/#_1","text":"","title":""},{"location":"sql/#calculate-difference-in-dates-hive-sql","text":"1 2 3 4 datediff ( to_date ( String column_with_timestamp_1 ), to_date ( String column_with_timestamp_1 ) )","title":"Calculate difference in dates (HIVE SQL)"},{"location":"sql/#list-all-columns-in-a-table-sql","text":"1 DESCRIBE name_of_table ;","title":"List all columns in a table (SQL)"},{"location":"sql/#if-clause-sql","text":"1 IF ( col_name IS NULL , 'Missing' , col_name ) AS new_col_name ,","title":"IF clause (SQL)"},{"location":"sql/#case-clause-sql","text":"1 2 3 4 5 6 CASE WHEN column_name_1 = 'condition_1' THEN 'result_1' WHEN column_name_2 LIKE 'begins_with_some_text%' THEN 'result_2' WHEN column_name_3 LIKE 'begins_with_some_text%' THEN 'result_3' ELSE 'Other' END AS new_column_name ,","title":"CASE clause (SQL)"}]}